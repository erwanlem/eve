{
    "function": "average",
    "asm": [
        {
            "type": [
                "float",
                "float"
            ],
            "instr": [
                "vbroadcastss 0x0(%rip),%zmm2",
                "vmulps %zmm2,%zmm1,%zmm1",
                "vfmadd132ps %zmm2,%zmm1,%zmm0"
            ]
        },
        {
            "type": [
                "signed char",
                "signed char"
            ],
            "instr": [
                "push %rbp",
                "vpxord %zmm0,%zmm1,%zmm2",
                "vpextrb $0x0,%xmm2,%eax",
                "mov %rsp,%rbp",
                "push %r15",
                "movsbl %al,%eax",
                "sar %eax",
                "push %r14",
                "vextracti128 $0x1,%ymm2,%xmm3",
                "vmovdqa64 %zmm0,%zmm6",
                "push %r13",
                "vpandd %zmm6,%zmm1,%zmm1",
                "push %r12",
                "push %rbx",
                "and $0xffffffffffffffc0,%rsp",
                "sub $0x48,%rsp",
                "mov %eax,0x44(%rsp)",
                "vpextrb $0x1,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x40(%rsp)",
                "vpextrb $0x2,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x3c(%rsp)",
                "vpextrb $0x3,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x38(%rsp)",
                "vpextrb $0x4,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x34(%rsp)",
                "vpextrb $0x5,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x30(%rsp)",
                "vpextrb $0x6,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x2c(%rsp)",
                "vpextrb $0x7,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x28(%rsp)",
                "vpextrb $0x8,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x24(%rsp)",
                "vpextrb $0x9,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x20(%rsp)",
                "vpextrb $0xa,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x1c(%rsp)",
                "vpextrb $0xb,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x18(%rsp)",
                "vpextrb $0xc,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x14(%rsp)",
                "vpextrb $0xd,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x10(%rsp)",
                "vpextrb $0xe,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0xc(%rsp)",
                "vpextrb $0xf,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x8(%rsp)",
                "vpextrb $0x0,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x4(%rsp)",
                "vpextrb $0x1,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,(%rsp)",
                "vpextrb $0x2,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x4(%rsp)",
                "vpextrb $0x3,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x8(%rsp)",
                "vpextrb $0x4,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0xc(%rsp)",
                "vpextrb $0x5,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x10(%rsp)",
                "vpextrb $0x6,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x14(%rsp)",
                "vpextrb $0x7,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x18(%rsp)",
                "vpextrb $0x8,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x1c(%rsp)",
                "vpextrb $0x9,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x20(%rsp)",
                "vpextrb $0xa,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x24(%rsp)",
                "vpextrb $0xb,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x28(%rsp)",
                "vpextrb $0xc,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x2c(%rsp)",
                "vpextrb $0xd,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x30(%rsp)",
                "vpextrb $0xe,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x34(%rsp)",
                "vpextrb $0xf,%xmm3,%eax",
                "movsbl %al,%eax",
                "vextracti64x4 $0x1,%zmm2,%ymm2",
                "sar %eax",
                "kmovd %eax,%k7",
                "vpextrb $0x0,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x38(%rsp)",
                "vpextrb $0x1,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k5",
                "vpextrb $0x2,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x3c(%rsp)",
                "vpextrb $0x3,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k6",
                "vpextrb $0x4,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x40(%rsp)",
                "vpextrb $0x5,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k3",
                "vpextrb $0x6,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x44(%rsp)",
                "vpextrb $0x7,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k2",
                "vpextrb $0x8,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x48(%rsp)",
                "vpextrb $0x9,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k1",
                "vpextrb $0xa,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x4c(%rsp)",
                "vpextrb $0xb,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k0",
                "vpextrb $0xc,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x50(%rsp)",
                "vpextrb $0xe,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "vpextrb $0xd,%xmm2,%r15d",
                "vpextrb $0xf,%xmm2,%r14d",
                "vextracti128 $0x1,%ymm2,%xmm2",
                "mov %eax,-0x54(%rsp)",
                "vpextrb $0x0,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x58(%rsp)",
                "vpextrb $0xc,%xmm2,%esi",
                "vpextrb $0x2,%xmm2,%eax",
                "movsbl %al,%eax",
                "movsbl %sil,%esi",
                "sar %eax",
                "sar %esi",
                "vpextrb $0xb,%xmm2,%edx",
                "mov %eax,-0x5c(%rsp)",
                "mov %esi,-0x64(%rsp)",
                "vpextrb $0x8,%xmm2,%eax",
                "vpextrb $0xd,%xmm2,%esi",
                "vpextrb $0xe,%xmm2,%r9d",
                "movsbl %dl,%edx",
                "sar %edx",
                "movsbl %sil,%esi",
                "movsbl %r9b,%r9d",
                "movsbl %al,%eax",
                "sar %eax",
                "vmovd 0x3c(%rsp),%xmm9",
                "vmovd 0x34(%rsp),%xmm4",
                "vmovd 0x2c(%rsp),%xmm8",
                "vmovd 0x24(%rsp),%xmm0",
                "vmovd 0x1c(%rsp),%xmm7",
                "vpextrb $0x1,%xmm2,%r13d",
                "vpextrb $0x3,%xmm2,%r12d",
                "vpextrb $0x4,%xmm2,%ecx",
                "vpextrb $0x5,%xmm2,%ebx",
                "vpextrb $0x6,%xmm2,%r8d",
                "vpextrb $0x7,%xmm2,%r11d",
                "vpextrb $0x9,%xmm2,%r10d",
                "vpextrb $0xa,%xmm2,%edi",
                "vmovd 0x14(%rsp),%xmm3",
                "kmovd %edx,%k4",
                "mov %esi,%edx",
                "mov %r9d,%esi",
                "vpextrb $0xf,%xmm2,%r9d",
                "vmovd 0x44(%rsp),%xmm2",
                "vpinsrb $0x1,0x38(%rsp),%xmm9,%xmm9",
                "vpinsrb $0x1,0x28(%rsp),%xmm8,%xmm8",
                "vpinsrb $0x1,0x18(%rsp),%xmm7,%xmm7",
                "mov %eax,-0x60(%rsp)",
                "vpinsrb $0x1,0x40(%rsp),%xmm2,%xmm2",
                "vpinsrb $0x1,0x30(%rsp),%xmm4,%xmm4",
                "vpinsrb $0x1,0x20(%rsp),%xmm0,%xmm0",
                "vpinsrb $0x1,0x10(%rsp),%xmm3,%xmm3",
                "vmovd 0xc(%rsp),%xmm5",
                "vpinsrb $0x1,0x8(%rsp),%xmm5,%xmm5",
                "vpunpcklwd %xmm8,%xmm4,%xmm4",
                "vpunpcklwd %xmm5,%xmm3,%xmm3",
                "vpunpcklwd %xmm9,%xmm2,%xmm2",
                "vpunpcklwd %xmm7,%xmm0,%xmm0",
                "vpunpckldq %xmm4,%xmm2,%xmm2",
                "vpunpckldq %xmm3,%xmm0,%xmm0",
                "vmovd 0x4(%rsp),%xmm4",
                "vmovd -0x4(%rsp),%xmm10",
                "vmovd -0xc(%rsp),%xmm5",
                "vmovd -0x14(%rsp),%xmm9",
                "vmovd -0x24(%rsp),%xmm8",
                "vmovd -0x2c(%rsp),%xmm3",
                "vpunpcklqdq %xmm0,%xmm2,%xmm0",
                "vmovd -0x1c(%rsp),%xmm2",
                "vpinsrb $0x1,-0x8(%rsp),%xmm10,%xmm10",
                "vpinsrb $0x1,-0x18(%rsp),%xmm9,%xmm9",
                "vpinsrb $0x1,-0x28(%rsp),%xmm8,%xmm8",
                "vpinsrb $0x1,(%rsp),%xmm4,%xmm4",
                "vpinsrb $0x1,-0x10(%rsp),%xmm5,%xmm5",
                "vpinsrb $0x1,-0x20(%rsp),%xmm2,%xmm2",
                "vpinsrb $0x1,-0x30(%rsp),%xmm3,%xmm3",
                "vmovd -0x34(%rsp),%xmm7",
                "kmovb %k7,%eax",
                "vpinsrb $0x1,%eax,%xmm7,%xmm7",
                "vpunpcklwd %xmm9,%xmm5,%xmm5",
                "vpunpcklwd %xmm7,%xmm3,%xmm3",
                "vpunpcklwd %xmm10,%xmm4,%xmm4",
                "vpunpcklwd %xmm8,%xmm2,%xmm2",
                "vpunpckldq %xmm3,%xmm2,%xmm2",
                "vpunpckldq %xmm5,%xmm4,%xmm4",
                "movsbl %r15b,%r15d",
                "movsbl %r14b,%r14d",
                "sar %r15d",
                "sar %r14d",
                "vmovd -0x3c(%rsp),%xmm11",
                "vmovd -0x40(%rsp),%xmm7",
                "vmovd -0x44(%rsp),%xmm10",
                "vmovd -0x48(%rsp),%xmm3",
                "vmovd -0x4c(%rsp),%xmm9",
                "vmovd -0x50(%rsp),%xmm5",
                "vmovd -0x54(%rsp),%xmm8",
                "vpunpcklqdq %xmm2,%xmm4,%xmm4",
                "vmovd -0x38(%rsp),%xmm2",
                "vpinsrb $0x1,%r14d,%xmm8,%xmm8",
                "vpinsrb $0x1,%r15d,%xmm5,%xmm5",
                "kmovb %k5,%eax",
                "vpinsrb $0x1,%eax,%xmm2,%xmm2",
                "kmovb %k6,%eax",
                "vpinsrb $0x1,%eax,%xmm11,%xmm11",
                "kmovb %k3,%eax",
                "vpinsrb $0x1,%eax,%xmm7,%xmm7",
                "kmovb %k2,%eax",
                "vpinsrb $0x1,%eax,%xmm10,%xmm10",
                "kmovb %k1,%eax",
                "vpinsrb $0x1,%eax,%xmm3,%xmm3",
                "kmovb %k0,%eax",
                "vpinsrb $0x1,%eax,%xmm9,%xmm9",
                "vpunpcklwd %xmm10,%xmm7,%xmm7",
                "vpunpcklwd %xmm8,%xmm5,%xmm5",
                "vpunpcklwd %xmm11,%xmm2,%xmm2",
                "vpunpcklwd %xmm9,%xmm3,%xmm3",
                "movsbl %r13b,%r13d",
                "movsbl %r12b,%r12d",
                "movsbl %cl,%ecx",
                "movsbl %bl,%ebx",
                "movsbl %r8b,%r8d",
                "movsbl %r11b,%r11d",
                "movsbl %r10b,%r10d",
                "movsbl %dil,%edi",
                "sar %r13d",
                "sar %r12d",
                "sar %ebx",
                "vpunpckldq %xmm5,%xmm3,%xmm3",
                "sar %ecx",
                "sar %r8d",
                "sar %r11d",
                "sar %r10d",
                "sar %edi",
                "sar %edx",
                "vpunpckldq %xmm7,%xmm2,%xmm2",
                "sar %esi",
                "movsbl %r9b,%r9d",
                "vmovd -0x5c(%rsp),%xmm12",
                "vmovd -0x60(%rsp),%xmm5",
                "vmovd -0x64(%rsp),%xmm7",
                "vpunpcklqdq %xmm3,%xmm2,%xmm2",
                "sar %r9d",
                "vmovd -0x58(%rsp),%xmm3",
                "vmovd %ecx,%xmm8",
                "vmovd %r8d,%xmm11",
                "vmovd %edi,%xmm10",
                "vmovd %esi,%xmm9",
                "vpinsrb $0x1,%r13d,%xmm3,%xmm3",
                "vpinsrb $0x1,%r12d,%xmm12,%xmm12",
                "vpinsrb $0x1,%ebx,%xmm8,%xmm8",
                "vpinsrb $0x1,%r11d,%xmm11,%xmm11",
                "lea -0x28(%rbp),%rsp",
                "vpinsrb $0x1,%r10d,%xmm5,%xmm5",
                "vpinsrb $0x1,%edx,%xmm7,%xmm7",
                "vpinsrb $0x1,%r9d,%xmm9,%xmm9",
                "kmovb %k4,%eax",
                "vpinsrb $0x1,%eax,%xmm10,%xmm10",
                "pop %rbx",
                "vpunpcklwd %xmm12,%xmm3,%xmm3",
                "vpunpcklwd %xmm11,%xmm8,%xmm8",
                "vpunpcklwd %xmm10,%xmm5,%xmm5",
                "vpunpcklwd %xmm9,%xmm7,%xmm7",
                "pop %r12",
                "vpunpckldq %xmm8,%xmm3,%xmm3",
                "vpunpckldq %xmm7,%xmm5,%xmm5",
                "pop %r13",
                "vpunpcklqdq %xmm5,%xmm3,%xmm3",
                "pop %r14",
                "vinserti128 $0x1,%xmm4,%ymm0,%ymm0",
                "vinserti128 $0x1,%xmm3,%ymm2,%ymm2",
                "vinserti64x4 $0x1,%ymm2,%zmm0,%zmm0",
                "pop %r15",
                "vpaddb %zmm1,%zmm0,%zmm0",
                "pop %rbp"
            ]
        },
        {
            "type": [
                "char",
                "char"
            ],
            "instr": [
                "push %rbp",
                "vpxord %zmm0,%zmm1,%zmm2",
                "vpextrb $0x0,%xmm2,%eax",
                "mov %rsp,%rbp",
                "push %r15",
                "movsbl %al,%eax",
                "sar %eax",
                "push %r14",
                "vextracti128 $0x1,%ymm2,%xmm3",
                "vmovdqa64 %zmm0,%zmm6",
                "push %r13",
                "vpandd %zmm6,%zmm1,%zmm1",
                "push %r12",
                "push %rbx",
                "and $0xffffffffffffffc0,%rsp",
                "sub $0x48,%rsp",
                "mov %eax,0x44(%rsp)",
                "vpextrb $0x1,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x40(%rsp)",
                "vpextrb $0x2,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x3c(%rsp)",
                "vpextrb $0x3,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x38(%rsp)",
                "vpextrb $0x4,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x34(%rsp)",
                "vpextrb $0x5,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x30(%rsp)",
                "vpextrb $0x6,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x2c(%rsp)",
                "vpextrb $0x7,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x28(%rsp)",
                "vpextrb $0x8,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x24(%rsp)",
                "vpextrb $0x9,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x20(%rsp)",
                "vpextrb $0xa,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x1c(%rsp)",
                "vpextrb $0xb,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x18(%rsp)",
                "vpextrb $0xc,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x14(%rsp)",
                "vpextrb $0xd,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x10(%rsp)",
                "vpextrb $0xe,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0xc(%rsp)",
                "vpextrb $0xf,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x8(%rsp)",
                "vpextrb $0x0,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,0x4(%rsp)",
                "vpextrb $0x1,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,(%rsp)",
                "vpextrb $0x2,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x4(%rsp)",
                "vpextrb $0x3,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x8(%rsp)",
                "vpextrb $0x4,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0xc(%rsp)",
                "vpextrb $0x5,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x10(%rsp)",
                "vpextrb $0x6,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x14(%rsp)",
                "vpextrb $0x7,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x18(%rsp)",
                "vpextrb $0x8,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x1c(%rsp)",
                "vpextrb $0x9,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x20(%rsp)",
                "vpextrb $0xa,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x24(%rsp)",
                "vpextrb $0xb,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x28(%rsp)",
                "vpextrb $0xc,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x2c(%rsp)",
                "vpextrb $0xd,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x30(%rsp)",
                "vpextrb $0xe,%xmm3,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x34(%rsp)",
                "vpextrb $0xf,%xmm3,%eax",
                "movsbl %al,%eax",
                "vextracti64x4 $0x1,%zmm2,%ymm2",
                "sar %eax",
                "kmovd %eax,%k7",
                "vpextrb $0x0,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x38(%rsp)",
                "vpextrb $0x1,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k5",
                "vpextrb $0x2,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x3c(%rsp)",
                "vpextrb $0x3,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k6",
                "vpextrb $0x4,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x40(%rsp)",
                "vpextrb $0x5,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k3",
                "vpextrb $0x6,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x44(%rsp)",
                "vpextrb $0x7,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k2",
                "vpextrb $0x8,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x48(%rsp)",
                "vpextrb $0x9,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k1",
                "vpextrb $0xa,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x4c(%rsp)",
                "vpextrb $0xb,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "kmovd %eax,%k0",
                "vpextrb $0xc,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x50(%rsp)",
                "vpextrb $0xe,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "vpextrb $0xd,%xmm2,%r15d",
                "vpextrb $0xf,%xmm2,%r14d",
                "vextracti128 $0x1,%ymm2,%xmm2",
                "mov %eax,-0x54(%rsp)",
                "vpextrb $0x0,%xmm2,%eax",
                "movsbl %al,%eax",
                "sar %eax",
                "mov %eax,-0x58(%rsp)",
                "vpextrb $0xc,%xmm2,%esi",
                "vpextrb $0x2,%xmm2,%eax",
                "movsbl %al,%eax",
                "movsbl %sil,%esi",
                "sar %eax",
                "sar %esi",
                "vpextrb $0xb,%xmm2,%edx",
                "mov %eax,-0x5c(%rsp)",
                "mov %esi,-0x64(%rsp)",
                "vpextrb $0x8,%xmm2,%eax",
                "vpextrb $0xd,%xmm2,%esi",
                "vpextrb $0xe,%xmm2,%r9d",
                "movsbl %dl,%edx",
                "sar %edx",
                "movsbl %sil,%esi",
                "movsbl %r9b,%r9d",
                "movsbl %al,%eax",
                "sar %eax",
                "vmovd 0x3c(%rsp),%xmm9",
                "vmovd 0x34(%rsp),%xmm4",
                "vmovd 0x2c(%rsp),%xmm8",
                "vmovd 0x24(%rsp),%xmm0",
                "vmovd 0x1c(%rsp),%xmm7",
                "vpextrb $0x1,%xmm2,%r13d",
                "vpextrb $0x3,%xmm2,%r12d",
                "vpextrb $0x4,%xmm2,%ecx",
                "vpextrb $0x5,%xmm2,%ebx",
                "vpextrb $0x6,%xmm2,%r8d",
                "vpextrb $0x7,%xmm2,%r11d",
                "vpextrb $0x9,%xmm2,%r10d",
                "vpextrb $0xa,%xmm2,%edi",
                "vmovd 0x14(%rsp),%xmm3",
                "kmovd %edx,%k4",
                "mov %esi,%edx",
                "mov %r9d,%esi",
                "vpextrb $0xf,%xmm2,%r9d",
                "vmovd 0x44(%rsp),%xmm2",
                "vpinsrb $0x1,0x38(%rsp),%xmm9,%xmm9",
                "vpinsrb $0x1,0x28(%rsp),%xmm8,%xmm8",
                "vpinsrb $0x1,0x18(%rsp),%xmm7,%xmm7",
                "mov %eax,-0x60(%rsp)",
                "vpinsrb $0x1,0x40(%rsp),%xmm2,%xmm2",
                "vpinsrb $0x1,0x30(%rsp),%xmm4,%xmm4",
                "vpinsrb $0x1,0x20(%rsp),%xmm0,%xmm0",
                "vpinsrb $0x1,0x10(%rsp),%xmm3,%xmm3",
                "vmovd 0xc(%rsp),%xmm5",
                "vpinsrb $0x1,0x8(%rsp),%xmm5,%xmm5",
                "vpunpcklwd %xmm8,%xmm4,%xmm4",
                "vpunpcklwd %xmm5,%xmm3,%xmm3",
                "vpunpcklwd %xmm9,%xmm2,%xmm2",
                "vpunpcklwd %xmm7,%xmm0,%xmm0",
                "vpunpckldq %xmm4,%xmm2,%xmm2",
                "vpunpckldq %xmm3,%xmm0,%xmm0",
                "vmovd 0x4(%rsp),%xmm4",
                "vmovd -0x4(%rsp),%xmm10",
                "vmovd -0xc(%rsp),%xmm5",
                "vmovd -0x14(%rsp),%xmm9",
                "vmovd -0x24(%rsp),%xmm8",
                "vmovd -0x2c(%rsp),%xmm3",
                "vpunpcklqdq %xmm0,%xmm2,%xmm0",
                "vmovd -0x1c(%rsp),%xmm2",
                "vpinsrb $0x1,-0x8(%rsp),%xmm10,%xmm10",
                "vpinsrb $0x1,-0x18(%rsp),%xmm9,%xmm9",
                "vpinsrb $0x1,-0x28(%rsp),%xmm8,%xmm8",
                "vpinsrb $0x1,(%rsp),%xmm4,%xmm4",
                "vpinsrb $0x1,-0x10(%rsp),%xmm5,%xmm5",
                "vpinsrb $0x1,-0x20(%rsp),%xmm2,%xmm2",
                "vpinsrb $0x1,-0x30(%rsp),%xmm3,%xmm3",
                "vmovd -0x34(%rsp),%xmm7",
                "kmovb %k7,%eax",
                "vpinsrb $0x1,%eax,%xmm7,%xmm7",
                "vpunpcklwd %xmm9,%xmm5,%xmm5",
                "vpunpcklwd %xmm7,%xmm3,%xmm3",
                "vpunpcklwd %xmm10,%xmm4,%xmm4",
                "vpunpcklwd %xmm8,%xmm2,%xmm2",
                "vpunpckldq %xmm3,%xmm2,%xmm2",
                "vpunpckldq %xmm5,%xmm4,%xmm4",
                "movsbl %r15b,%r15d",
                "movsbl %r14b,%r14d",
                "sar %r15d",
                "sar %r14d",
                "vmovd -0x3c(%rsp),%xmm11",
                "vmovd -0x40(%rsp),%xmm7",
                "vmovd -0x44(%rsp),%xmm10",
                "vmovd -0x48(%rsp),%xmm3",
                "vmovd -0x4c(%rsp),%xmm9",
                "vmovd -0x50(%rsp),%xmm5",
                "vmovd -0x54(%rsp),%xmm8",
                "vpunpcklqdq %xmm2,%xmm4,%xmm4",
                "vmovd -0x38(%rsp),%xmm2",
                "vpinsrb $0x1,%r14d,%xmm8,%xmm8",
                "vpinsrb $0x1,%r15d,%xmm5,%xmm5",
                "kmovb %k5,%eax",
                "vpinsrb $0x1,%eax,%xmm2,%xmm2",
                "kmovb %k6,%eax",
                "vpinsrb $0x1,%eax,%xmm11,%xmm11",
                "kmovb %k3,%eax",
                "vpinsrb $0x1,%eax,%xmm7,%xmm7",
                "kmovb %k2,%eax",
                "vpinsrb $0x1,%eax,%xmm10,%xmm10",
                "kmovb %k1,%eax",
                "vpinsrb $0x1,%eax,%xmm3,%xmm3",
                "kmovb %k0,%eax",
                "vpinsrb $0x1,%eax,%xmm9,%xmm9",
                "vpunpcklwd %xmm10,%xmm7,%xmm7",
                "vpunpcklwd %xmm8,%xmm5,%xmm5",
                "vpunpcklwd %xmm11,%xmm2,%xmm2",
                "vpunpcklwd %xmm9,%xmm3,%xmm3",
                "movsbl %r13b,%r13d",
                "movsbl %r12b,%r12d",
                "movsbl %cl,%ecx",
                "movsbl %bl,%ebx",
                "movsbl %r8b,%r8d",
                "movsbl %r11b,%r11d",
                "movsbl %r10b,%r10d",
                "movsbl %dil,%edi",
                "sar %r13d",
                "sar %r12d",
                "sar %ebx",
                "vpunpckldq %xmm5,%xmm3,%xmm3",
                "sar %ecx",
                "sar %r8d",
                "sar %r11d",
                "sar %r10d",
                "sar %edi",
                "sar %edx",
                "vpunpckldq %xmm7,%xmm2,%xmm2",
                "sar %esi",
                "movsbl %r9b,%r9d",
                "vmovd -0x5c(%rsp),%xmm12",
                "vmovd -0x60(%rsp),%xmm5",
                "vmovd -0x64(%rsp),%xmm7",
                "vpunpcklqdq %xmm3,%xmm2,%xmm2",
                "sar %r9d",
                "vmovd -0x58(%rsp),%xmm3",
                "vmovd %ecx,%xmm8",
                "vmovd %r8d,%xmm11",
                "vmovd %edi,%xmm10",
                "vmovd %esi,%xmm9",
                "vpinsrb $0x1,%r13d,%xmm3,%xmm3",
                "vpinsrb $0x1,%r12d,%xmm12,%xmm12",
                "vpinsrb $0x1,%ebx,%xmm8,%xmm8",
                "vpinsrb $0x1,%r11d,%xmm11,%xmm11",
                "lea -0x28(%rbp),%rsp",
                "vpinsrb $0x1,%r10d,%xmm5,%xmm5",
                "vpinsrb $0x1,%edx,%xmm7,%xmm7",
                "vpinsrb $0x1,%r9d,%xmm9,%xmm9",
                "kmovb %k4,%eax",
                "vpinsrb $0x1,%eax,%xmm10,%xmm10",
                "pop %rbx",
                "vpunpcklwd %xmm12,%xmm3,%xmm3",
                "vpunpcklwd %xmm11,%xmm8,%xmm8",
                "vpunpcklwd %xmm10,%xmm5,%xmm5",
                "vpunpcklwd %xmm9,%xmm7,%xmm7",
                "pop %r12",
                "vpunpckldq %xmm8,%xmm3,%xmm3",
                "vpunpckldq %xmm7,%xmm5,%xmm5",
                "pop %r13",
                "vpunpcklqdq %xmm5,%xmm3,%xmm3",
                "pop %r14",
                "vinserti128 $0x1,%xmm4,%ymm0,%ymm0",
                "vinserti128 $0x1,%xmm3,%ymm2,%ymm2",
                "vinserti64x4 $0x1,%ymm2,%zmm0,%zmm0",
                "pop %r15",
                "vpaddb %zmm1,%zmm0,%zmm0",
                "pop %rbp"
            ]
        },
        {
            "type": [
                "double",
                "double"
            ],
            "instr": [
                "vbroadcastsd 0x0(%rip),%zmm2",
                "vmulpd %zmm2,%zmm1,%zmm1",
                "vfmadd132pd %zmm2,%zmm1,%zmm0"
            ]
        },
        {
            "type": [
                "int",
                "int"
            ],
            "instr": [
                "vpxord %zmm0,%zmm1,%zmm2",
                "vpsrad $0x1,%zmm2,%zmm2",
                "vpandd %zmm0,%zmm1,%zmm1",
                "vpaddd %zmm2,%zmm1,%zmm0"
            ]
        }
    ]
}