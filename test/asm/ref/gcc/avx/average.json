{
    "function": "average",
    "asm": [
        {
            "type": [
                "float",
                "float"
            ],
            "instr": [
                "vmovaps 0x0(%rip),%ymm2",
                "vmulps %ymm2,%ymm1,%ymm1",
                "vmulps %ymm2,%ymm0,%ymm0",
                "vaddps %ymm0,%ymm1,%ymm0"
            ]
        },
        {
            "type": [
                "signed char",
                "signed char"
            ],
            "instr": [
                "push %rbp",
                "vxorps %ymm1,%ymm0,%ymm2",
                "vandps %ymm1,%ymm0,%ymm0",
                "vpextrb $0x3,%xmm2,%edx",
                "vextractf128 $0x1,%ymm2,%xmm1",
                "vpextrb $0x0,%xmm2,%eax",
                "movsbl %dl,%edx",
                "vpextrb $0x2,%xmm2,%r9d",
                "vmovdqa %xmm0,%xmm4",
                "movsbl %al,%eax",
                "mov %rsp,%rbp",
                "push %r15",
                "sar %edx",
                "vpextrb $0x1,%xmm2,%r15d",
                "push %r14",
                "movsbl %r9b,%r9d",
                "sar %eax",
                "movsbl %r15b,%r15d",
                "push %r13",
                "sar %r9d",
                "sar %r15d",
                "vextractf128 $0x1,%ymm0,%xmm3",
                "push %r12",
                "vpextrb $0x5,%xmm1,%r14d",
                "vmovd %eax,%xmm6",
                "vpextrb $0x6,%xmm1,%r8d",
                "push %rbx",
                "vpextrb $0x7,%xmm1,%r13d",
                "vmovd %r9d,%xmm9",
                "vpextrb $0x9,%xmm1,%r12d",
                "vpextrb $0xa,%xmm1,%edi",
                "vpextrb $0xb,%xmm1,%ebx",
                "movsbl %r8b,%r8d",
                "movsbl %r14b,%r14d",
                "vpextrb $0xc,%xmm1,%ecx",
                "vpextrb $0xd,%xmm1,%r11d",
                "movsbl %dil,%edi",
                "sar %r8d",
                "and $0xffffffffffffffe0,%rsp",
                "vpextrb $0xe,%xmm1,%esi",
                "movsbl %cl,%ecx",
                "sar %edi",
                "mov %edx,-0x4(%rsp)",
                "vpextrb $0x4,%xmm2,%edx",
                "movsbl %sil,%esi",
                "movsbl %dl,%edx",
                "vpextrb $0xf,%xmm1,%r10d",
                "sar %ecx",
                "movsbl %r13b,%r13d",
                "sar %edx",
                "vpinsrb $0x1,-0x4(%rsp),%xmm9,%xmm9",
                "sar %esi",
                "movsbl %r12b,%r12d",
                "mov %edx,-0x8(%rsp)",
                "vpextrb $0x5,%xmm2,%edx",
                "movsbl %bl,%ebx",
                "sar %r14d",
                "movsbl %dl,%edx",
                "vpinsrb $0x1,%r15d,%xmm6,%xmm6",
                "movsbl %r11b,%r11d",
                "sar %r13d",
                "sar %edx",
                "vpunpcklwd %xmm9,%xmm6,%xmm6",
                "movsbl %r10b,%r10d",
                "sar %r12d",
                "mov %edx,-0xc(%rsp)",
                "vpextrb $0x6,%xmm2,%edx",
                "sar %ebx",
                "vmovd %r8d,%xmm9",
                "movsbl %dl,%edx",
                "sar %r11d",
                "vpinsrb $0x1,%r13d,%xmm9,%xmm9",
                "sar %edx",
                "mov %edx,-0x10(%rsp)",
                "vpextrb $0x7,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x14(%rsp)",
                "vpextrb $0x8,%xmm2,%edx",
                "vmovd -0x10(%rsp),%xmm8",
                "vpinsrb $0x1,-0x14(%rsp),%xmm8,%xmm8",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x18(%rsp)",
                "vpextrb $0x9,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x1c(%rsp)",
                "vpextrb $0xa,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x20(%rsp)",
                "vpextrb $0xb,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x24(%rsp)",
                "vpextrb $0xc,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x28(%rsp)",
                "vpextrb $0xd,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x2c(%rsp)",
                "vpextrb $0xe,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x30(%rsp)",
                "vpextrb $0xf,%xmm2,%edx",
                "vmovd -0x8(%rsp),%xmm2",
                "vpinsrb $0x1,-0xc(%rsp),%xmm2,%xmm2",
                "movsbl %dl,%edx",
                "sar %edx",
                "vpunpcklwd %xmm8,%xmm2,%xmm2",
                "vmovd %edi,%xmm8",
                "mov %edx,-0x34(%rsp)",
                "vpextrb $0x0,%xmm1,%edx",
                "vpunpckldq %xmm2,%xmm6,%xmm6",
                "vpinsrb $0x1,%ebx,%xmm8,%xmm8",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x38(%rsp)",
                "vpextrb $0x1,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x3c(%rsp)",
                "vpextrb $0x2,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x40(%rsp)",
                "vpextrb $0x3,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x44(%rsp)",
                "vpextrb $0x4,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x48(%rsp)",
                "vpextrb $0x8,%xmm1,%edx",
                "vmovd -0x18(%rsp),%xmm1",
                "vpinsrb $0x1,-0x1c(%rsp),%xmm1,%xmm1",
                "vmovd -0x20(%rsp),%xmm7",
                "vmovd -0x28(%rsp),%xmm0",
                "vmovd -0x30(%rsp),%xmm5",
                "movsbl %dl,%edx",
                "vpinsrb $0x1,-0x24(%rsp),%xmm7,%xmm7",
                "sar %edx",
                "vmovd -0x38(%rsp),%xmm2",
                "vmovd -0x40(%rsp),%xmm10",
                "vpinsrb $0x1,-0x34(%rsp),%xmm5,%xmm5",
                "vpinsrb $0x1,-0x2c(%rsp),%xmm0,%xmm0",
                "sar %r10d",
                "vpunpcklwd %xmm7,%xmm1,%xmm1",
                "vpinsrb $0x1,-0x3c(%rsp),%xmm2,%xmm2",
                "vmovd %esi,%xmm7",
                "vpunpcklwd %xmm5,%xmm0,%xmm0",
                "vpinsrb $0x1,-0x44(%rsp),%xmm10,%xmm10",
                "vmovd -0x48(%rsp),%xmm5",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vmovd %ecx,%xmm0",
                "vpinsrb $0x1,%r14d,%xmm5,%xmm5",
                "vpunpcklqdq %xmm1,%xmm6,%xmm6",
                "vmovd %edx,%xmm1",
                "vpinsrb $0x1,%r11d,%xmm0,%xmm0",
                "vpinsrb $0x1,%r12d,%xmm1,%xmm1",
                "vpinsrb $0x1,%r10d,%xmm7,%xmm7",
                "vpunpcklwd %xmm10,%xmm2,%xmm2",
                "vpunpcklwd %xmm7,%xmm0,%xmm0",
                "vpunpcklwd %xmm9,%xmm5,%xmm5",
                "vpunpcklwd %xmm8,%xmm1,%xmm1",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vpunpckldq %xmm5,%xmm2,%xmm2",
                "lea -0x28(%rbp),%rsp",
                "vpunpcklqdq %xmm1,%xmm2,%xmm1",
                "pop %rbx",
                "pop %r12",
                "vinsertf128 $0x1,%xmm1,%ymm6,%ymm1",
                "pop %r13",
                "pop %r14",
                "vmovdqa %xmm1,%xmm0",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "pop %r15",
                "pop %rbp",
                "vpaddb %xmm0,%xmm4,%xmm0",
                "vpaddb %xmm1,%xmm3,%xmm3",
                "vinsertf128 $0x1,%xmm3,%ymm0,%ymm0"
            ]
        },
        {
            "type": [
                "char",
                "char"
            ],
            "instr": [
                "push %rbp",
                "vxorps %ymm1,%ymm0,%ymm2",
                "vandps %ymm1,%ymm0,%ymm0",
                "vpextrb $0x3,%xmm2,%edx",
                "vextractf128 $0x1,%ymm2,%xmm1",
                "vpextrb $0x0,%xmm2,%eax",
                "movsbl %dl,%edx",
                "vpextrb $0x2,%xmm2,%r9d",
                "vmovdqa %xmm0,%xmm4",
                "movsbl %al,%eax",
                "mov %rsp,%rbp",
                "push %r15",
                "sar %edx",
                "vpextrb $0x1,%xmm2,%r15d",
                "push %r14",
                "movsbl %r9b,%r9d",
                "sar %eax",
                "movsbl %r15b,%r15d",
                "push %r13",
                "sar %r9d",
                "sar %r15d",
                "vextractf128 $0x1,%ymm0,%xmm3",
                "push %r12",
                "vpextrb $0x5,%xmm1,%r14d",
                "vmovd %eax,%xmm6",
                "vpextrb $0x6,%xmm1,%r8d",
                "push %rbx",
                "vpextrb $0x7,%xmm1,%r13d",
                "vmovd %r9d,%xmm9",
                "vpextrb $0x9,%xmm1,%r12d",
                "vpextrb $0xa,%xmm1,%edi",
                "vpextrb $0xb,%xmm1,%ebx",
                "movsbl %r8b,%r8d",
                "movsbl %r14b,%r14d",
                "vpextrb $0xc,%xmm1,%ecx",
                "vpextrb $0xd,%xmm1,%r11d",
                "movsbl %dil,%edi",
                "sar %r8d",
                "and $0xffffffffffffffe0,%rsp",
                "vpextrb $0xe,%xmm1,%esi",
                "movsbl %cl,%ecx",
                "sar %edi",
                "mov %edx,-0x4(%rsp)",
                "vpextrb $0x4,%xmm2,%edx",
                "movsbl %sil,%esi",
                "movsbl %dl,%edx",
                "vpextrb $0xf,%xmm1,%r10d",
                "sar %ecx",
                "movsbl %r13b,%r13d",
                "sar %edx",
                "vpinsrb $0x1,-0x4(%rsp),%xmm9,%xmm9",
                "sar %esi",
                "movsbl %r12b,%r12d",
                "mov %edx,-0x8(%rsp)",
                "vpextrb $0x5,%xmm2,%edx",
                "movsbl %bl,%ebx",
                "sar %r14d",
                "movsbl %dl,%edx",
                "vpinsrb $0x1,%r15d,%xmm6,%xmm6",
                "movsbl %r11b,%r11d",
                "sar %r13d",
                "sar %edx",
                "vpunpcklwd %xmm9,%xmm6,%xmm6",
                "movsbl %r10b,%r10d",
                "sar %r12d",
                "mov %edx,-0xc(%rsp)",
                "vpextrb $0x6,%xmm2,%edx",
                "sar %ebx",
                "vmovd %r8d,%xmm9",
                "movsbl %dl,%edx",
                "sar %r11d",
                "vpinsrb $0x1,%r13d,%xmm9,%xmm9",
                "sar %edx",
                "mov %edx,-0x10(%rsp)",
                "vpextrb $0x7,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x14(%rsp)",
                "vpextrb $0x8,%xmm2,%edx",
                "vmovd -0x10(%rsp),%xmm8",
                "vpinsrb $0x1,-0x14(%rsp),%xmm8,%xmm8",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x18(%rsp)",
                "vpextrb $0x9,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x1c(%rsp)",
                "vpextrb $0xa,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x20(%rsp)",
                "vpextrb $0xb,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x24(%rsp)",
                "vpextrb $0xc,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x28(%rsp)",
                "vpextrb $0xd,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x2c(%rsp)",
                "vpextrb $0xe,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x30(%rsp)",
                "vpextrb $0xf,%xmm2,%edx",
                "vmovd -0x8(%rsp),%xmm2",
                "vpinsrb $0x1,-0xc(%rsp),%xmm2,%xmm2",
                "movsbl %dl,%edx",
                "sar %edx",
                "vpunpcklwd %xmm8,%xmm2,%xmm2",
                "vmovd %edi,%xmm8",
                "mov %edx,-0x34(%rsp)",
                "vpextrb $0x0,%xmm1,%edx",
                "vpunpckldq %xmm2,%xmm6,%xmm6",
                "vpinsrb $0x1,%ebx,%xmm8,%xmm8",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x38(%rsp)",
                "vpextrb $0x1,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x3c(%rsp)",
                "vpextrb $0x2,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x40(%rsp)",
                "vpextrb $0x3,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x44(%rsp)",
                "vpextrb $0x4,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sar %edx",
                "mov %edx,-0x48(%rsp)",
                "vpextrb $0x8,%xmm1,%edx",
                "vmovd -0x18(%rsp),%xmm1",
                "vpinsrb $0x1,-0x1c(%rsp),%xmm1,%xmm1",
                "vmovd -0x20(%rsp),%xmm7",
                "vmovd -0x28(%rsp),%xmm0",
                "vmovd -0x30(%rsp),%xmm5",
                "movsbl %dl,%edx",
                "vpinsrb $0x1,-0x24(%rsp),%xmm7,%xmm7",
                "sar %edx",
                "vmovd -0x38(%rsp),%xmm2",
                "vmovd -0x40(%rsp),%xmm10",
                "vpinsrb $0x1,-0x34(%rsp),%xmm5,%xmm5",
                "vpinsrb $0x1,-0x2c(%rsp),%xmm0,%xmm0",
                "sar %r10d",
                "vpunpcklwd %xmm7,%xmm1,%xmm1",
                "vpinsrb $0x1,-0x3c(%rsp),%xmm2,%xmm2",
                "vmovd %esi,%xmm7",
                "vpunpcklwd %xmm5,%xmm0,%xmm0",
                "vpinsrb $0x1,-0x44(%rsp),%xmm10,%xmm10",
                "vmovd -0x48(%rsp),%xmm5",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vmovd %ecx,%xmm0",
                "vpinsrb $0x1,%r14d,%xmm5,%xmm5",
                "vpunpcklqdq %xmm1,%xmm6,%xmm6",
                "vmovd %edx,%xmm1",
                "vpinsrb $0x1,%r11d,%xmm0,%xmm0",
                "vpinsrb $0x1,%r12d,%xmm1,%xmm1",
                "vpinsrb $0x1,%r10d,%xmm7,%xmm7",
                "vpunpcklwd %xmm10,%xmm2,%xmm2",
                "vpunpcklwd %xmm7,%xmm0,%xmm0",
                "vpunpcklwd %xmm9,%xmm5,%xmm5",
                "vpunpcklwd %xmm8,%xmm1,%xmm1",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vpunpckldq %xmm5,%xmm2,%xmm2",
                "lea -0x28(%rbp),%rsp",
                "vpunpcklqdq %xmm1,%xmm2,%xmm1",
                "pop %rbx",
                "pop %r12",
                "vinsertf128 $0x1,%xmm1,%ymm6,%ymm1",
                "pop %r13",
                "pop %r14",
                "vmovdqa %xmm1,%xmm0",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "pop %r15",
                "pop %rbp",
                "vpaddb %xmm0,%xmm4,%xmm0",
                "vpaddb %xmm1,%xmm3,%xmm3",
                "vinsertf128 $0x1,%xmm3,%ymm0,%ymm0"
            ]
        },
        {
            "type": [
                "double",
                "double"
            ],
            "instr": [
                "vmovapd 0x0(%rip),%ymm2",
                "vmulpd %ymm2,%ymm1,%ymm1",
                "vmulpd %ymm2,%ymm0,%ymm0",
                "vaddpd %ymm0,%ymm1,%ymm0"
            ]
        },
        {
            "type": [
                "int",
                "int"
            ],
            "instr": [
                "vxorps %ymm1,%ymm0,%ymm2",
                "vmovdqa %ymm1,%ymm6",
                "vmovd %xmm2,%eax",
                "vpextrd $0x1,%xmm2,%esi",
                "vpextrd $0x3,%xmm2,%edx",
                "sar %eax",
                "sar %esi",
                "vandps %ymm6,%ymm0,%ymm0",
                "mov %eax,%ecx",
                "vpextrd $0x2,%xmm2,%eax",
                "vextractf128 $0x1,%ymm2,%xmm2",
                "sar %edx",
                "vpextrd $0x1,%xmm2,%edi",
                "sar %eax",
                "vmovd %ecx,%xmm7",
                "vmovdqa %xmm0,%xmm3",
                "vmovd %eax,%xmm5",
                "vmovd %xmm2,%eax",
                "sar %edi",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "sar %eax",
                "vpinsrd $0x1,%edx,%xmm5,%xmm5",
                "vmovd %eax,%xmm1",
                "vpextrd $0x2,%xmm2,%eax",
                "sar %eax",
                "vpinsrd $0x1,%edi,%xmm1,%xmm1",
                "vmovd %eax,%xmm4",
                "vpextrd $0x3,%xmm2,%eax",
                "vpinsrd $0x1,%esi,%xmm7,%xmm2",
                "sar %eax",
                "vpunpcklqdq %xmm5,%xmm2,%xmm2",
                "vpinsrd $0x1,%eax,%xmm4,%xmm4",
                "vpaddd %xmm2,%xmm3,%xmm2",
                "vpunpcklqdq %xmm4,%xmm1,%xmm1",
                "vpaddd %xmm1,%xmm0,%xmm0",
                "vinsertf128 $0x1,%xmm0,%ymm2,%ymm0"
            ]
        }
    ]
}